{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BSON: @load\n",
    "using Flux\n",
    "using Flux: chunk\n",
    "using Flux.Data: DataLoader\n",
    "using ImageFiltering\n",
    "using Images\n",
    "using ImageIO\n",
    "using MLDatasets: FashionMNIST\n",
    "using LinearAlgebra\n",
    "using MLDatasets\n",
    "using Plots\n",
    "using JLD\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the problem of recovering $x\\in\\mathbb{R}^n$ from noisy signed compressive measurements of the form\n",
    "\n",
    "$$y = \\text{sign}(Ax) + \\epsilon, $$\n",
    "\n",
    "where $\\epsilon\\in\\mathbb{R}^n$ is noise and $A \\in \\mathbb{R}^{m\\times n}$ is a known sensing matrix. We assume the unknown signal $x$ lives in the range of known a generative model $G:\\mathbb{R}^k \\rightarrow \\mathbb{R}^n$, i.e. $x = G(z)$ for some $z \\in \\mathbb{R}^k$. We assume the generative model $G$ is  fully-connected feedforward network of the form \n",
    "\n",
    "$$ G(x) = \\sigma_d(A_d\\sigma_{d-1}(A_{d-1} \\cdots \\sigma_1(A_1 z)\\cdots)),$$\n",
    "\n",
    "where $A_i \\in \\mathbb{R}^{n_i \\times n_{i-1}}$ is the weight matrix and $\\sigma_i$ is the activation function correpsonding to the $i\\text{th}$ layer of $G$. Thus, the task of recovering $x$ can be reduced to recovering the corresponding $z$ such that $G(z) = x$. \n",
    "\n",
    "\n",
    "We solve this problem using the following iterative algorithm called Partially Linearized Updates for Generative Inversion (PLUGIn)-1bit:\n",
    "\n",
    "$$x^{k+1} = x^k -\\eta A_1^{\\top}\\cdots A_d^{\\top}A^{\\top}\\left(\\text{sign}(AG(x^k)) - y \\right) .$$\n",
    "\n",
    "Here, $\\eta$ is the stepsize that depends on the weight matrices and the activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now list the function that are relavant to the computation of the esitmate using the PLUGIn-1bit algorithm. The following function can be used to output an estimate $x^{k+1}$ from PLUGIn-1bit from another estimate $x^k$ of the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the plugin Iterate\n",
    "function PLUGIN_1bitCS_estimate(A, G, W, y, z, stepsize)\n",
    "    d = W'*A'* (sign.(A * G(z)) - y )\n",
    "    return z - stepsize * d\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of PLUGIn algorithm in general is that it allows us to pre-multiply the weight matrices. The following function is used to compute $A_d\\cdot A_{d-1}\\cdots A_1$ given a generative model $G$. It also normalizes each weight matrix w.r.t. its top singular value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalized_weight_product(G)\n",
    "    (_, z_dim) = size(Flux.params(G[1])[1]);\n",
    "    W = I(z_dim)\n",
    "    for i in 1:length(G)\n",
    "        _, s, _ = svd(Flux.params(G[i])[1])\n",
    "        W = Flux.params(G[i])[1] * W /s[1]\n",
    "    end\n",
    "    return W\n",
    "end  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function can be used to create a general generative function with number of nodes in each layer specified. The entries of the weight matrices are initialized to be normalized random normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_network(net_param)\n",
    "    n_0 = net_param[1]\n",
    "    n_1 = net_param[2]\n",
    "    L = Chain(Dense(n_0, n_1, relu; initW =(out,in) ->  randn(n_1, n_0)/sqrt(n_1)))\n",
    "\n",
    "    for i in 2:length(net_param)-1\n",
    "        n_0 = net_param[i]\n",
    "        n_1 = net_param[i+1]\n",
    "        L = Chain(L, Dense(n_0, n_1, relu; initW =(out,in) ->  randn(n_1, n_0)/sqrt(n_1)))\n",
    "    end\n",
    "    return L\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function can be used to estimate the ground truth from signed compressive measurements using the PLUGIn-1bit algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function PLUGIn_onebitCS(G, y, A, max_iter, stepsize, tolerance,lambda, out_toggle)\n",
    "    \n",
    "    (_, z_dim) = size(Flux.params(G[1])[1]);\n",
    "    (m, _) = size(A)\n",
    "    W = I(z_dim)\n",
    "  \n",
    "    #normalize the weights of the network\n",
    "    for i in 1:length(G)\n",
    "        _, s, _ = svd(Flux.params(G[i])[1])\n",
    "        W = Flux.params(G[i])[1] * W /s[1]\n",
    "    end\n",
    "  \n",
    "    z = randn(z_dim)\n",
    "    iter = 1\n",
    "    succ_error = 1\n",
    "  \n",
    "    while iter <= max_iter && succ_error > tolerance\n",
    "      # d gives the PLUGIn direction\n",
    "      d = sign.(A * G(z)) - y\n",
    "      d = W'*A'* d\n",
    "      z -= stepsize * d\n",
    "      succ_error = norm(stepsize * d)\n",
    "      if iter % out_toggle == 0  \n",
    "          println(\"====> In quasi-gradient: Iteration: $iter Successive error: $succ_error\")\n",
    "      end\n",
    "      iter += 1\n",
    "    end\n",
    "    println(\"====> In quasi-gradient: Iteration: $iter Successive error: $succ_error\")\n",
    "  \n",
    "    return z\n",
    "  end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first experiment, we want to examine the convergence rate of the PLUGIn-bit algorithm as a function of the number of measurement. Define the recovery error  as:\n",
    "$$\\text{recovery error, $\\delta$} := \\left\\| \\tfrac{x^{\\natural}}{\\|x^\\natural\\|} - \\tfrac{x^*}{\\|x^*\\|} \\right\\|,$$\n",
    "where $x^\\natural$ is the recovered estimate of ground truth $x^*$. We hope to see a error decay as $\\delta \\sim \\tfrac{1}{m^\\alpha}$ for some $\\alpha \\in (0,1)$.\n",
    "\n",
    "\n",
    "The following code segment is used to generate the relationship betwen $\\delta$ and $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING** The following code segment can take a while to complete as it will compute the recovery error multiple times for various measurement levels. To see a instance of the result, skip to the next code segment that loads the reults of a pre-compiled instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_param = [5, 200, 750]\n",
    "x_dim  = net_param[length(net_param)]\n",
    "z_dim = net_param[1]\n",
    "\n",
    "G = create_network(net_param)\n",
    "    \n",
    "stepsize = 1\n",
    "max_iter = 2000\n",
    "tolerance = 1e-7\n",
    "\n",
    "W = normalized_weight_product(G)\n",
    "m_list = 750:1000:50000\n",
    "trials = 10\n",
    "recov_error_matrix = zeros(length(m_list))\n",
    "\n",
    "for trial in 1:trials\n",
    "    recov_error = []\n",
    "    z = randn(z_dim)\n",
    "\n",
    "    for m in m_list\n",
    "        A = randn(m, x_dim)/sqrt(m)\n",
    "        y = sign.(A*G(z))\n",
    "        z_est = zeros(z_dim)\n",
    "        iter = 1\n",
    "        succ_error = 1\n",
    "\n",
    "        while iter <= max_iter && succ_error > tolerance\n",
    "            z_old = z_est\n",
    "            z_est = PLUGIN_1bitCS_estimate(A, G, W, y, z_est, stepsize)\n",
    "            succ_error = norm(z_old - z_est, 2)\n",
    "            iter += 1\n",
    "        end\n",
    "        push!(recov_error, norm(z/norm(z) - z_est/norm(z_est)))\n",
    "    end\n",
    "    recov_error_matrix = hcat(recov_error_matrix, recov_error)\n",
    "\n",
    "end\n",
    "\n",
    "# change the filename before running this code if you wish to save the results\n",
    "\n",
    "# M = recov_error_matrix[:,2:end]\n",
    "# save(\"result/error_1.jld\", \"error\", M, \"measurement\", m_list, \"trials\", trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now show the relationship between recovery error and measurement level. Note that the plot is in log scale and so we hope to see a linear curve. In this plot, the generative function $G$ a 2-layer deep random neural network and satisfies $5\\rightarrow 200\\rightarrow 750$. The number of measurment ranges is uniformly sampled in the interval [750, 30000]. For each measurment, we report the average of 10 trials. \n",
    "\n",
    "Note that the relationship between recovery error and measurement level \"looks\" linear (in log scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2 = load(\"result/error.jld\", \"error\")\n",
    "m_list = load(\"result/error.jld\", \"measurement\")\n",
    "\n",
    "average_recov = mean(M2, dims=2)\n",
    "std_recov = std(M2, dims=2)\n",
    "plot(log.(m_list), average_recov + std_recov, fillrange = average_recov - std_recov,fillalpha = 0.35, c = 1, label = false)\n",
    "plot!(log.(m_list), average_recov, label = \"recovery error\", xlabel = \"number of measurement in log scale\", ylabel = \"average recovery error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code segment can be used to estimate the ground truth using the function PLUGIn_onebitCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "G = Chain(\n",
    "    Dense(5, 75, relu, bias = false; initW =(out,in) ->  randn(75, 5)/sqrt(75)),\n",
    "    Dense(75, 100, relu, bias = false; initW =(out,in) -> randn(75, 75)/sqrt(75)),\n",
    "    Dense(75, 150, relu, bias = false; initW =(out,in) -> randn(150, 75)/sqrt(150))\n",
    ")\n",
    "\n",
    "\n",
    "z = randn(5)\n",
    "m = 10000; A = randn(m, 150)/sqrt(m)\n",
    "y = sign.(A*G(z)) + 1e-14 * randn(m)\n",
    "\n",
    "stepsize = 1\n",
    "tolerance = 1e-14\n",
    "max_iter = 10000\n",
    "out_toggle = 1000\n",
    "lambda = 100\n",
    "z_rec = PLUGIn_onebitCS(G,y,A, max_iter, stepsize, tolerance, lambda, out_toggle)\n",
    "recov_error = norm(z/norm(z) - z_rec/norm(z_rec))\n",
    "recon_error = norm(G(z) - G(z_rec))\n",
    "println(\"recovery error: $recov_error, reconstruction error: $recon_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code segment is an attempt to implement another algorithm for 1-bit CS with generative model introduced in the paper:\n",
    "https://arxiv.org/pdf/1908.05368.pdf \n",
    "\n",
    "This still needs to be debugged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Zygote to solve ERM for a synthetic problem\n",
    "\n",
    "\n",
    "G = Chain(\n",
    "    Dense(5, 75, relu, bias = false; initW =(out,in) ->  randn(75, 5)/sqrt(75)),\n",
    "    Dense(75, 75, relu, bias = false; initW =(out,in) -> randn(75, 75)/sqrt(75)),\n",
    "    Dense(75, 150, relu, bias = false; initW =(out,in) -> randn(150, 75)/sqrt(150))\n",
    ")\n",
    "\n",
    "\n",
    "z = randn(5)\n",
    "m = 5000; A = randn(m, 150)/sqrt(m)\n",
    "y = sign.(A*G(z)) + 1e-14 * randn(m)\n",
    "\n",
    "stepsize = .1\n",
    "tolerance = 1e-7\n",
    "max_iter = 1000\n",
    "out_toggle = 1000\n",
    "lambda = 10\n",
    "\n",
    "z_rec = randn(5)\n",
    "\n",
    "\n",
    "for i in 1:max_iter\n",
    "\n",
    "    d = gradient(z_rec -> norm(G(z_rec),2)^2 - 2*lambda * y'*(A * G(z_rec))/m, z_rec)[1]\n",
    "    z_rec -= stepsize * d\n",
    "    succ_error = norm(stepsize * d)\n",
    "    if i % out_toggle == 0  \n",
    "        println(\"====> In quasi-gradient: Iteration: $i Successive error: $succ_error\")\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# z_rec = PLUGIn_onebitCS(G,y,A, max_iter, stepsize, tolerance, out_toggle)\n",
    "recov_error = norm(z/norm(z) - z_rec/norm(z_rec))\n",
    "recon_error = norm(G(z) - G(z_rec))\n",
    "println(\"recovery error: $recov_error, reconstruction error: $recon_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
