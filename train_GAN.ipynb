{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets: MNIST\n",
    "using Flux.Data: DataLoader\n",
    "using Flux\n",
    "using CUDA\n",
    "using Zygote\n",
    "using UnicodePlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.0002, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_g = 2e-4          # Learning rate of the generator network\n",
    "lr_d = 2e-4          # Learning rate of the discriminator network\n",
    "batch_size = 128    # batch size\n",
    "num_epochs = 10   # Number of epochs to train for\n",
    "output_period = 100 # Period length for plots of generator samples\n",
    "n_features = 28 * 28# Number of pixels in each sample of the MNIST dataset\n",
    "latent_dim = 100    # Dimension of latent space\n",
    "opt_dscr = ADAM(lr_d)# Optimizer for the discriminator\n",
    "opt_gen = ADAM(lr_g) # Optimizer for the generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Downloading\u001b[22m\u001b[39m artifact: CUDA_compat\n",
      "┌ Info: The GPU function is being called but the GPU is not accessible. \n",
      "│ Defaulting back to the CPU. (No action is required if you want to run on the CPU).\n",
      "└ @ Flux /project/def-mpf/bjoshi/.julia/packages/Flux/qAdFM/src/functor.jl:187\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_x, _ = MNIST.traindata(Float32);\n",
    "# This dataset has pixel values ∈ [0:1]. Map these to [-1:1]\n",
    "train_x = 2f0 * reshape(train_x, 28, 28, 1, :) .- 1f0 |>gpu;\n",
    "# DataLoader allows to access data batch-wise and handles shuffling.\n",
    "train_loader = DataLoader(train_x, batchsize=batch_size, shuffle=true);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784, 1024, #3),                 \u001b[90m# 803_840 parameters\u001b[39m\n",
       "  Dropout(0.3),\n",
       "  Dense(1024, 512, #4),                 \u001b[90m# 524_800 parameters\u001b[39m\n",
       "  Dropout(0.3),\n",
       "  Dense(512, 256, #5),                  \u001b[90m# 131_328 parameters\u001b[39m\n",
       "  Dropout(0.3),\n",
       "  Dense(256, 1, σ),                     \u001b[90m# 257 parameters\u001b[39m\n",
       ")\u001b[90m                   # Total: 8 arrays, \u001b[39m1_460_225 parameters, 5.571 MiB."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "discriminator = Chain(Dense(n_features, 1024, x -> leakyrelu(x, 0.2f0)),\n",
    "                        Dropout(0.3),\n",
    "                        Dense(1024, 512, x -> leakyrelu(x, 0.2f0)),\n",
    "                        Dropout(0.3),\n",
    "                        Dense(512, 256, x -> leakyrelu(x, 0.2f0)),\n",
    "                        Dropout(0.3),\n",
    "                        Dense(256, 1, sigmoid)) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(100, 256, #9),                  \u001b[90m# 25_856 parameters\u001b[39m\n",
       "  Dense(256, 512, #10),                 \u001b[90m# 131_584 parameters\u001b[39m\n",
       "  Dense(512, 1024, #11),                \u001b[90m# 525_312 parameters\u001b[39m\n",
       "  Dense(1024, 784, tanh),               \u001b[90m# 803_600 parameters\u001b[39m\n",
       ")\u001b[90m                   # Total: 8 arrays, \u001b[39m1_486_352 parameters, 5.670 MiB."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = Chain(Dense(latent_dim, 256, x -> leakyrelu(x, 0.2f0)),\n",
    "                    Dense(256, 512, x -> leakyrelu(x, 0.2f0)),\n",
    "                    Dense(512, 1024, x -> leakyrelu(x, 0.2f0)),\n",
    "                    Dense(1024, n_features, tanh)) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_dscr! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train_dscr!(discriminator, real_data, fake_data)\n",
    "    this_batch = size(real_data)[end] # Number of samples in the batch\n",
    "    # Concatenate real and fake data into one big vector\n",
    "    all_data = hcat(real_data, fake_data)\n",
    "\n",
    "    # Target vector for predictions: 1 for real data, 0 for fake data.\n",
    "    all_target = [ones(eltype(real_data), 1, this_batch) zeros(eltype(fake_data), 1, this_batch)] |> gpu;\n",
    "\n",
    "    ps = Flux.params(discriminator)\n",
    "    loss, pullback = Zygote.pullback(ps) do\n",
    "        preds = discriminator(all_data)\n",
    "        loss = Flux.Losses.binarycrossentropy(preds, all_target)\n",
    "    end\n",
    "    # To get the gradients we evaluate the pullback with 1.0 as a seed gradient.\n",
    "    grads = pullback(1f0)\n",
    "\n",
    "    # Update the parameters of the discriminator with the gradients we calculated above\n",
    "    Flux.update!(opt_dscr, Flux.params(discriminator), grads)\n",
    "    \n",
    "    return loss \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_gen! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train_gen!(discriminator, generator)\n",
    "    # Sample noise\n",
    "    noise = randn(latent_dim, batch_size) |> gpu;\n",
    "\n",
    "    # Define parameters and get the pullback\n",
    "    ps = Flux.params(generator)\n",
    "    # Evaluate the loss function while calculating the pullback. We get the loss for free\n",
    "    loss, back = Zygote.pullback(ps) do\n",
    "        preds = discriminator(generator(noise));\n",
    "        loss = Flux.Losses.binarycrossentropy(preds, 1.) \n",
    "    end\n",
    "    # Evaluate the pullback with a seed-gradient of 1.0 to get the gradients for\n",
    "    # the parameters of the generator\n",
    "    grads = back(1.0f0)\n",
    "    Flux.update!(opt_gen, Flux.params(generator), grads)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossvec_gen = zeros(num_epochs)\n",
    "lossvec_dscr = zeros(num_epochs)\n",
    "\n",
    "for n in 1:num_epochs\n",
    "    loss_sum_gen = 0.0f0\n",
    "    loss_sum_dscr = 0.0f0\n",
    "\n",
    "    for x in train_loader\n",
    "        # - Flatten the images from 28x28xbatchsize to 784xbatchsize\n",
    "        real_data = flatten(x);\n",
    "\n",
    "        # Train the discriminator\n",
    "        noise = randn(latent_dim, size(x)[end]) |> gpu\n",
    "        fake_data = generator(noise)\n",
    "        loss_dscr = train_dscr!(discriminator, real_data, fake_data)\n",
    "        loss_sum_dscr += loss_dscr\n",
    "\n",
    "        # Train the generator\n",
    "        loss_gen = train_gen!(discriminator, generator)\n",
    "        loss_sum_gen += loss_gen\n",
    "    end\n",
    "\n",
    "    # Add the per-sample loss of the generator and discriminator\n",
    "    lossvec_gen[n] = loss_sum_gen / size(train_x)[end]\n",
    "    lossvec_dscr[n] = loss_sum_dscr / size(train_x)[end]\n",
    "\n",
    "    if n % output_period == 0\n",
    "        @show n\n",
    "        noise = randn(latent_dim, 4) |> gpu;\n",
    "        fake_data = reshape(generator(noise), 28, 4*28);\n",
    "        p = heatmap(fake_data, colormap=:inferno)\n",
    "        print(p)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
